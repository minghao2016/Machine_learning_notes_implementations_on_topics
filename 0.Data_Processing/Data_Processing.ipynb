{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missiong Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['France', 44.0, 72000.0],\n",
       "        ['Spain', 27.0, 48000.0],\n",
       "        ['Germany', 30.0, 54000.0],\n",
       "        ['Spain', 38.0, 61000.0],\n",
       "        ['Germany', 40.0, nan]], dtype=object),\n",
       " array(['No', 'Yes', 'No', 'No', 'Yes'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv('./Data.csv')\n",
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "X[:5,:],y[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking care of missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imputer=imputer.fit(X[:,1:3])\n",
    "X[:,1:3]=imputer.transform(X[:,1:3])\n",
    "X[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ﬁt** part is used to extract some info of the data on which the object is applied (here, Imputer will\n",
    "spot the missing values and get the mean of the column). Then, the **transform** part is used to apply some\n",
    "transformation (here, Imputer will replace the missing value by the mean).\n",
    "\n",
    "\n",
    "## Is replacing by the mean the best strategy to handle missing values?\n",
    "If for example you have a lot of missing values, then mean substitution is not the best thing. \n",
    "\n",
    "Other strategies include \"median\" imputation, \"most frequent\" imputation\n",
    "or prediction imputation. \n",
    "\n",
    "**Prediction imputation**: \n",
    "\n",
    "You take your feature column that contains the missing values and you set this feature column as the dependent variable, while setting the other columns as the independent variables.\n",
    "\n",
    "Then you split your dataset into a Training set and a Test set where the Training set contains all the observations (the lines) where your feature column that you just set as the dependent variable has no missing value and the Test set contains all the observations where your dependent variable column contains the missing values.\n",
    "\n",
    "Then you perform a classiﬁcation model (a good one for this situation is k-NN) to predict the missing values in the test set. And eventually you replace your missing values by the predictions. A great strategy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Data\n",
    "Since machine learning models are based on mathematical equations, so that's why we need to encode the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 44.0, 72000.0],\n",
       "        [2, 27.0, 48000.0],\n",
       "        [1, 30.0, 54000.0],\n",
       "        [2, 38.0, 61000.0],\n",
       "        [1, 40.0, 63777.77777777778]], dtype=object),\n",
       " array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "X[:,0]=labelencoder.fit_transform(X[:,0])\n",
    "labelencoder_y=LabelEncoder()\n",
    "y=labelencoder_y.fit_transform(y)\n",
    "X[:5,:],y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is a problem!*\n",
    "\n",
    "These are actually three categories and there is no relational order between the three.\n",
    "\n",
    "We cannot compare France Spain and Germany by saying that Spain is greater than Germany or Germany is greater than France.\n",
    "\n",
    "> Dummy variables!\n",
    "\n",
    "## Dummy Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nancy/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/nancy/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.40000000e+01,\n",
       "        7.20000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 2.70000000e+01,\n",
       "        4.80000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.00000000e+01,\n",
       "        5.40000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.80000000e+01,\n",
       "        6.10000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.00000000e+01,\n",
       "        6.37777778e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.50000000e+01,\n",
       "        5.80000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.87777778e+01,\n",
       "        5.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.80000000e+01,\n",
       "        7.90000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 5.00000000e+01,\n",
       "        8.30000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.70000000e+01,\n",
       "        6.70000000e+04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder=OneHotEncoder(categorical_features=[0])\n",
    "X=onehotencoder.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,  y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the diﬀerence between the training set and the test set?\n",
    "The training set is a subset of your data on which your model will learn how to predict the dependent\n",
    "variable with the independent variables. The test set is the complimentary subset from the training set, on\n",
    "which you will evaluate your model to see if it manages to predict correctly the dependent variable with the\n",
    "independent variables.\n",
    "#### Why do we split on the dependent variable?\n",
    "Because we want to have well distributed values of the dependent variable in the training and test set. For\n",
    "example if we only had the same value of the dependent variable in the training set, our model wouldn’t be\n",
    "able to learn any correlation between the independent and dependent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "**Why we need feature scaling?**\n",
    "A lot of machine learning models are based on Euclidean distance (the square root of the sum of the square coordinates). Without feature scaling, the Euclidean distance will be dominated by one feature. So we absolutely need to put the variables in the same scale.\n",
    "\n",
    "Even if sometimes machine models are not based on Euclidean distances we will still need to do features scaling because the algorithm will converge much faster.\n",
    "\n",
    "\n",
    "two types:\n",
    "1. standardization\n",
    "\n",
    "\\begin{equation}\n",
    "x_{stand}=\\frac{x-mean(x)}{std(x)}\n",
    "\\end{equation}\n",
    "\n",
    "> rescales data to have a mean (μ) of 0 and standard deviation (σ) of 1 (unit variance).\n",
    "\n",
    "\n",
    "2. normalization\n",
    "\n",
    "\\begin{equation}\n",
    "x_{norm}=\\frac{x-min(x)}{max(x)-min(x)}\n",
    "\\end{equation}\n",
    "\n",
    ">rescales the values into a range of [0,1]\n",
    "\n",
    "#### Do we really have to apply Feature Scaling on the dummy variables?\n",
    "Yes, if you want to optimize the accuracy of your model predictions.\n",
    "No, if you want to keep the most interpretation as possible in your model.\n",
    "\n",
    "#### When should we use Standardization and Normalization?\n",
    "Generally you should normalize (normalization) when the data is normally distributed, and scale (standard-\n",
    "ization) when the data is not normally distributed. In doubt, you should go for standardization. However what is commonly done is that the two scaling methods are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X=StandardScaler()\n",
    "X_train=sc_X.fit_transform(X_train)\n",
    "X_test=sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to fit the object to X_train first so that X_train and X_test are scales on the same basis. \n",
    "\n",
    "X_test is the same as the features getting on X_train simply because the object StandardScaler was fitted to X_train.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
