{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA V.S. Logistic Regression**:\n",
    "    \n",
    "1. When the classes are well-separated, the parameter estimates for the\n",
    "logistic regression model are surprisingly unstable. Linear discriminant\n",
    "analysis does not suffer from this problem.\n",
    "\n",
    "2. If n is small and the distribution of the predictors X is approximately\n",
    "normal in each of the classes, the linear discriminant model is again\n",
    "more stable than the logistic regression model.\n",
    "\n",
    "3. Linear discriminant analysis is popular\n",
    "when we have more than two response classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bayes’ Theorem for Classification\n",
    "Suppose that we wish to classify an observation into one of K classes, where\n",
    "K ≥ 2.\n",
    "\n",
    "**Prior**:Let $\\pi_k=Pr(Y=k)$ represent the overall or ***prior***\n",
    "probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the kth\n",
    "category of the response variable Y . \n",
    "\n",
    "Let $f_k(X) ≡ Pr(X = x|Y = k)$ denote\n",
    "the ***density function*** of X for an observation that comes from the kth class. In other words, fk(x) is relatively large if there is a high probability that an observation in the kth class has X ≈ x.\n",
    "\n",
    "**Bayes’\n",
    "theorem** states that\n",
    "\n",
    "\\begin{align}\n",
    "Pr(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \n",
    "\\end{align}\n",
    "\n",
    "**Posterior**:$p_k(X)\n",
    "= Pr(Y = k|X)$ an observation X = x belongs to the kth class, given the predictor value for that\n",
    "observation\n",
    "\n",
    "**Estimating $π_k$:** simply compute the fraction of the training\n",
    "observations that belong to the kth class.\n",
    "\n",
    "**Estimating $f_k(X)$:** more challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis for p = 1\n",
    "\n",
    "Assume p = 1—that is, we have only one predictor. We\n",
    "would like to obtain an estimate for $f_k(x)$ that we can estimate $p_k(x)$. We will then classify an observation to the class\n",
    "for which $p_k(x)$ is greatest. \n",
    "\n",
    "## Assumptions\n",
    "In order to estimate $f_k(x)$, we will first make\n",
    "some assumptions about its form:\n",
    "\n",
    "1. Assume that $f_k(x)$ is normal or Gaussian.\n",
    "\\begin{align}\n",
    "f_k(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp{\\left( -\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2 \\right)}\n",
    "\\end{align}\n",
    "\n",
    "where $μ_k$ and $σ_k^2$ are the mean and variance parameters for the kth class.\n",
    "\n",
    "2. Assume that $\\sigma_1^2=...=\\sigma_k^2$\n",
    ": that is, there is a shared\n",
    "variance term across all K classes, which for simplicity we can denote by\n",
    "$\\sigma^2$.\n",
    "\n",
    "So\n",
    "\\begin{align}\n",
    "p_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 \\right)}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_l)^2 \\right)}}\n",
    "\\end{align}\n",
    "\n",
    "The Bayes classifier involves assigning an observation X = x to the class for which $p_k(x)$ is largest. Taking the log of $p_k(x)$\n",
    "and rearranging the terms, it is not hard to show that this is equivalent to\n",
    "assigning the observation to the class for which\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k) \\quad\\quad (4.13)\n",
    "\\end{align}\n",
    "\n",
    "is largest.\n",
    "\n",
    "For instance, if K = 2 and π1 = π2, then the Bayes classifier\n",
    "assigns an observation to class 1 if $2x (μ_1 − μ_2) > μ^2_1\n",
    "− μ^2_2$, and to class\n",
    "2 otherwise. In this case, the Bayes decision boundary corresponds to the\n",
    "point where\n",
    "\n",
    "\\begin{align}\n",
    "x=\\frac{\\mu_1^2-\\mu_2^2}{2(\\mu_1-\\mu_2)}=\\frac{\\mu_1+\\mu_2}{2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Parameters Estimation\n",
    "\n",
    "In practice, even if we are quite certain of our assumption that X is drawn\n",
    "from a Gaussian distribution within each class, we still have to estimate\n",
    "the parameters $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$.\n",
    "\n",
    "\n",
    "**Linear discriminant analysis (LDA)** method approximates the Bayes classifier by plugging estimates for $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$ into (4.13)\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mu}_k=\\frac{1}{n_k}\\sum_{i:y_i=k}x_i  \\quad (4.15) \\\\\n",
    "\\hat{\\sigma}^2=\\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu_k})^2 \\quad (4.16)\\\\\n",
    "\\hat{\\pi_k}=\\frac{n_k}{n}\n",
    "\\end{align} \n",
    "\n",
    "\n",
    "where n is the total number of training observations, and $n_k$ is the number\n",
    "of training observations in the kth class. \n",
    "\n",
    "$\\hat{\\mu}_k$: average of all the training observations from the kth class;\n",
    "\n",
    "$\\hat{\\sigma}^2$: a weighted average of the sample variances for each of the K classes.\n",
    "\n",
    "$\\hat{\\pi_k}$: the proportion of the training observations\n",
    "that belong to the kth class\n",
    "\n",
    "\n",
    "##  LDA classifier\n",
    "The LDA classifier assigns an observation X = x to the class for which\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\delta}_k(x)=x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)\n",
    "\\end{align} \n",
    "\n",
    "is largest.\n",
    "\n",
    "The word ***linear*** in the classifier’s name stems from the fact\n",
    "that the ***discriminant functions*** $\\hat{\\delta}_k(x)$ are linear functions of x.\n",
    "\n",
    "<img src=\"./images/6.png\" width=600>\n",
    "\n",
    "The right-hand panel of Figure 4.4 displays a histogram of a random\n",
    "sample of 20 observations from each class. \n",
    "\n",
    "To implement LDA,\n",
    "\n",
    "1. Estimating πk, μk, and σ2 using (4.15) and (4.16).\n",
    "2. Compute the decision boundary, shown as a black solid line, that results from assigning an observation to the class for which $\\hat{\\delta}_k(x)$ is largest.\n",
    "\n",
    "In this case, since n1 = n2 = 20,\n",
    "we have $\\hat{\\pi_1}$ = $\\hat{\\pi_2}$. As a result, the decision boundary corresponds to the\n",
    "midpoint between the sample means for the two classes,$\\frac{\\mu_1+\\mu_2}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis for p >1 \n",
    "\n",
    "Assume that X = (X1,X2, . . .,Xp) is drawn from a **multivariate Gaussian** (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.\n",
    "\n",
    "\n",
    "## Multivariate Gaussian Distribution\n",
    "\n",
    "Assumes that each individual predictor\n",
    "follows a one-dimensional normal distribution with some\n",
    "correlation between each pair of predictors.\n",
    "\n",
    "<img src=\"./images/7.png\" width=600>\n",
    "\n",
    "To indicate that a p-dimensional random variable X has a multivariate\n",
    "Gaussian distribution, we write X ∼ N(μ,Σ). Here E(X) = μ is\n",
    "the mean of X (a vector with p components), and Cov(X) = Σ is the\n",
    "p × p **covariance matrix** of X. Formally, the **multivariate Gaussian density**\n",
    "is defined as\n",
    "\n",
    "\\begin{align}\n",
    "f(x)=\\frac{1}{\\sqrt{(2\\pi)^{p}|Σ|}}\\exp{\\left( \\frac{1}{2}(x-\\mu)^TΣ^{-1}(x-\\mu) \\right)}\n",
    "\\end{align} \n",
    "\n",
    "In the case of p > 1 predictors, the **LDA classifier** assumes that the\n",
    "observations in the kth class are drawn from a multivariate Gaussian distribution\n",
    "$N(μ_k,Σ)$, where $μ_k$ is a class-specific mean vector, and Σ is a\n",
    "covariance matrix that is common to all K classes.\n",
    "\n",
    "Plugging the density function for the kth class, $f_k(X = x)$, into $Pr(Y = k|X = x)$, the Bayes classifier assigns an observation X = x\n",
    "to the class for which\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x^TΣ^{-1}\\mu_k-\\frac{1}{2}\\mu_k^TΣ^{-1}\\mu_k+\\log{\\pi_k}\n",
    "\\end{align} \n",
    "\n",
    "is largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
